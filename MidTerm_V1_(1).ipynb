{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaturyaGajula/DATA603-Spring22-2274-Th/blob/main/MidTerm_V1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxCOugEbOW3b"
      },
      "source": [
        "In this project, we are trying to solve the sentiment classifier problem. Unlike other problems, we need to covert data into word embedings because, computer can only understand numbers and we need to convert words to numbers and categorize the text into one of the 12 categories provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM44PdLGDnJ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://github.com/msaricaumbc/DS_data/blob/master/ds602/dataset_newsletter.csv?raw=true')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK8UMCWoDr70"
      },
      "outputs": [],
      "source": [
        "df.drop(columns='Unnamed: 0', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfzoNyI9D2Oe"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDn11KRZD5M_"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okwV61BED658"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCW_26coD9_T"
      },
      "outputs": [],
      "source": [
        "#since Category has no null values, grouping the dataframe with Category\n",
        "df.groupby('category').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaPH-KL9EBkW"
      },
      "outputs": [],
      "source": [
        "title_null_counts = df.groupby('category')['title'].apply(lambda x: x.isnull().sum())\n",
        "print(title_null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvryUe32EL2k"
      },
      "outputs": [],
      "source": [
        "body_null_counts = df.groupby('category')['body'].apply(lambda x: x.isnull().sum())\n",
        "print(body_null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-NSBnWOEOJU"
      },
      "outputs": [],
      "source": [
        "signature_null_counts = df.groupby('category')['signature'].apply(lambda x: x.isnull().sum())\n",
        "print(signature_null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQHiTp4QEU_U"
      },
      "outputs": [],
      "source": [
        "# since the amount of null values in the data is negligible, we are dropping the rows with atleast one null values. \n",
        "#To cite another reason, based on the domain, 'body' can have a more impact compared to signature or title and the null contents in the body is less \n",
        "temp_df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCKnE85pEXlN"
      },
      "outputs": [],
      "source": [
        "temp_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GduBYMeVEb9e"
      },
      "outputs": [],
      "source": [
        "temp_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfzSs4sZEezZ"
      },
      "outputs": [],
      "source": [
        "# single dot appears for 129 times in 'body' column and does not contibute much to the model. Dropping the rows which has '.'\n",
        "value_to_remove = '.'\n",
        "mask = temp_df['body'] != value_to_remove\n",
        "filtered_df = temp_df[mask]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Rr3Bm2EiC9"
      },
      "outputs": [],
      "source": [
        "filtered_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSIsSd-TEpra"
      },
      "outputs": [],
      "source": [
        "filtered_df.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siTr-nXCWuab"
      },
      "outputs": [],
      "source": [
        "temp_df = pd.concat([filtered_df,week_processed], axis = 1)\n",
        "temp_df.columns= [*filtered_df.columns,'isweekend']\n",
        "temp_df.dropna().groupby('category')['isweekend'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjo_PHcVEsOt"
      },
      "outputs": [],
      "source": [
        "#we have the filtered data. Since 3 out of 4 coumns are text data, we have to convert the text data into word embeddings.\n",
        "#converting to word embeeddings\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oyvQxZ1Ew_m"
      },
      "outputs": [],
      "source": [
        "#using a model from hugging face library\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaaN0OnhE6oC"
      },
      "outputs": [],
      "source": [
        "filtered_df = filtered_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CANtVorKE_C-"
      },
      "outputs": [],
      "source": [
        "#Since bert model does all the preprocessing in backend, we do not want to preprocess the sentence like remove stop words, converting to lowercase, removing puntuations etc.\n",
        "body_embeddings = model.encode(filtered_df.body.astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_E4yHM0FOE2"
      },
      "outputs": [],
      "source": [
        "body_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2RHo9V1FPy6"
      },
      "outputs": [],
      "source": [
        "title_embeddings = model.encode(filtered_df.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOQedt5gFSG0"
      },
      "outputs": [],
      "source": [
        "title_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hVWGxdFXwY"
      },
      "outputs": [],
      "source": [
        "signature_embeddings = model.encode(filtered_df.signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA48dmJfFaUu"
      },
      "outputs": [],
      "source": [
        "signature_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "902EtZvNFjx0"
      },
      "outputs": [],
      "source": [
        "week_unprocessed = pd.to_datetime(filtered_df.submissiontime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqOkcY_8Fl6C"
      },
      "outputs": [],
      "source": [
        "week_unprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmH5dVSFFpEZ"
      },
      "outputs": [],
      "source": [
        "week_processed = week_unprocessed.dt.weekday.map(lambda x: 0 if x >= 5 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymTfhr7WFrrK"
      },
      "outputs": [],
      "source": [
        "week_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q7fMKhjZKyS"
      },
      "source": [
        "**written analysis of cleaned data for characteristics**\n",
        "- We have cleaned the data by removing null values and by removing the unnecessary values in the columns.\n",
        "- We have converted the Submission time column to differentiate between weekend or weekday to reduce computational overheads.\n",
        "- We have used BERT to convert words to word embeddings as it is an attention based model, and can give better results compared to TF-IDF or Count Vectorizer.\n",
        "\n",
        "**Note**\n",
        "Since BERT can handle all text related pre-processing work, we will not be performing any of the pre-processing steps explicitly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gay_QsAQb5wF"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=20)\n",
        "title_transformed = pca.fit_transform(title_embeddings)\n",
        "print(title_transformed.shape)\n",
        "body_transformed = pca.fit_transform(body_embeddings)\n",
        "print(body_transformed.shape)\n",
        "signature_transformed = pca.fit_transform(signature_embeddings)\n",
        "print(signature_transformed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V4AjJGYFu1T"
      },
      "outputs": [],
      "source": [
        "title_df = pd.DataFrame(title_transformed)#, columns=['Title_Column_1', 'Title_Column_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmWGW9wwFw-F"
      },
      "outputs": [],
      "source": [
        "body_df = pd.DataFrame(body_transformed)#, columns=['Body_Column_1', 'Body_Column_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nREurH6FzOE"
      },
      "outputs": [],
      "source": [
        "signature_df = pd.DataFrame(signature_transformed)#, columns=['Sign_Column_1', 'Sign_Column_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jGMkA1qF2cL"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([title_df, body_df,signature_df, week_processed], axis=1)#, keys=[title_df, body_df,signature_df])\n",
        "X.columns=X.columns.astype(str) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJpeKjxlF64S"
      },
      "outputs": [],
      "source": [
        "y = filtered_df.category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGSqWhPEF9hp"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8b4QI1BC8x0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5E-ZLarHT6E"
      },
      "outputs": [],
      "source": [
        "#X.columns = [x for x in range(0,16)]\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pcs', PCA()),\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBHn2SGOA9dW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'pcs__n_components' :[ 5,15,None],\n",
        "    'clf__penalty' : [None,'elasticnet']\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK6d_vZlBuvs"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, param_grid = param_grid, cv=5)\n",
        "grid_search.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1gm_pu2DGv9"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dfqpRDFDUbd"
      },
      "outputs": [],
      "source": [
        "#X.columns = [x for x in range(0,16)]\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pcs', PCA()),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3t_P_ZKDozc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'pcs__n_components' :[5,15,None],\n",
        "    'clf__n_estimators' : [25,100,None],\n",
        "    'clf__max_depth' : [5,10,None],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dfG4sdkD_4R"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, param_grid = param_grid, cv=5)\n",
        "grid_search.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_8fDxEzEFJd"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8-OaHcWJDf1"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_2tn7G0EFMI"
      },
      "outputs": [],
      "source": [
        "#X.columns = [x for x in range(0,16)]\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pcs', PCA()),\n",
        "    ('clf', SVC())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HArUczJvEFO2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'pcs__n_components' :[10, 20,None],\n",
        "    'clf__gamma' : [5,10,'scale'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjrCfodXLZMt"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, param_grid = param_grid, cv=3)\n",
        "grid_search.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0kwElNREFSJ"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmhbIU_DXv8n"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdTMQeoLYa0S"
      },
      "source": [
        "so far Support Vector Classifier is the good model according to the score. But also considering Random FOrest for F1 score as the accuracy is almost same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7iU6_hnHxVs"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100)\n",
        "forest.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5z4o6f3SpKB"
      },
      "outputs": [],
      "source": [
        "clf = SVC(gamma='scale') \n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JLVfla5Uzs8"
      },
      "outputs": [],
      "source": [
        "y_pred_clf = clf.predict(X_test)\n",
        "y_pred_forest = forest.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7fmIdfFHxYM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm_clf = confusion_matrix(y_test,y_pred_clf)\n",
        "cm_forest = confusion_matrix(y_test,y_pred_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxpp9SG6Hxb0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(cm_clf,annot=True,cmap = 'Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDcR0Q2ZZztX"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(cm_forest,annot=True,cmap = 'Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPwUaWpXNCd"
      },
      "source": [
        "It is evident that there are good amount of misclassifications between labels 1,10 and 3. Model is not able to better distinguish between "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luoVX3MiHxj_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test,y_pred_clf,average='weighted')\n",
        "print('F1 Score : ', f1*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoyvd74MZ82B"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test,y_pred_forest,average='weighted')\n",
        "print('F1 Score : ', f1*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZ4OoBmWS-E"
      },
      "source": [
        "We are choosing F1 Score here as our metrics because, we cannot take accuracy into account as the classes are highly imbalabced and it is not clear that we need to reduce true positives or flase potsitives. So we are choosing the harmonic mean of precision and recall i.e, F1 Score. So far with 77.8 F1 score, random forest is the best model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}